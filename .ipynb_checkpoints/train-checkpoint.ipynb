{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4333033c-e890-417f-8180-d38a700dcdcf",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import time\n",
    "from line_profiler import profile\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "import importlib\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import uproot\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm \n",
    "import random\n",
    "import lib.loss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from lib.dataset import ImageDataLoader\n",
    "from lib.model import UNet\n",
    "from lib.loss import FocalLoss\n",
    "from lib.common import set_seed\n",
    "from lib.config import ConfigLoader\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8372fba1",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def create_model(n_classes, weights, device, alpha=0.7, beta=0.3):\n",
    "    model = UNet(1, n_classes=n_classes, depth=4, n_filters=16)\n",
    "    loss_fn = TverskyLoss(alpha=alpha, beta=beta)\n",
    "    #loss_fn = nn.CrossEntropyLoss(weight=torch.tensor(weights, dtype=torch.float32, device=device))\n",
    "    #loss_fn = FocalLoss(alpha=torch.tensor(weights, dtype=torch.float32, device=device), gamma=2, reduction='mean')\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    return model, loss_fn, optim\n",
    "\n",
    "\n",
    "def get_class_weights(stats):\n",
    "    if np.any(stats == 0.):\n",
    "        idx = np.where(stats == 0.)\n",
    "        stats[idx] = 1\n",
    "        weights = 1. / stats\n",
    "        weights[idx] = 0\n",
    "    else:\n",
    "        weights = 1. / stats\n",
    "    return [weight / sum(weights) for weight in weights]\n",
    "\n",
    "def save_class_weights(weights, path=\"class_weights.npy\"):\n",
    "    np.save(path, weights)\n",
    "    print(\"\\033[34m-- Class weights saved\\033[0m\")\n",
    "\n",
    "def load_class_weights(path=\"class_weights.npy\"):\n",
    "    if os.path.exists(path):\n",
    "        print(\"\\033[34m-- Loading saved class weights\\033[0m\")\n",
    "        return np.load(path)\n",
    "    else:\n",
    "        print(\"\\033[35m-- Class weights file not found; calculating class weights\\033[0m\")\n",
    "        return None\n",
    "\n",
    "def calculate_metrics(pred, target, n_classes):\n",
    "    pred = pred.argmax(dim=1).flatten().cpu().numpy()  \n",
    "    target = target.flatten().cpu().numpy()  \n",
    "\n",
    "    accuracy = (pred == target).mean()\n",
    "    precision = precision_score(target, pred, labels=[1, 2], average='macro', zero_division=0)\n",
    "    recall = recall_score(target, pred, labels=[1, 2], average='macro', zero_division=0)\n",
    "\n",
    "    return accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "52448bdd-0af4-48c7-8cca-bcef45fa9a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m-- Loaded Configuration --\u001b[0m\n",
      "event_type: kshort_signal_overlay_new\n",
      "plane: U\n",
      "n_events: 40000\n",
      "file_title: kshort_new_20241106_training_output\n",
      "raw_dir: /gluster/data/microboone/strangeness/raw\n",
      "output_dir: /gluster/data/microboone/strangeness/processed\n",
      "input_dir: /gluster/data/microboone/strangeness/processed/kshort_signal_overlay_new/U/input\n",
      "target_dir: /gluster/data/microboone/strangeness/processed/kshort_signal_overlay_new/U/target\n",
      "height: 512\n",
      "width: 512\n",
      "train_pct: 0.8\n",
      "valid_pct: 0.2\n",
      "seed: 42\n",
      "batch_size: 8\n",
      "n_classes: 3\n",
      "n_epochs: 10\n",
      "model_name: kshort_unet_model\n"
     ]
    }
   ],
   "source": [
    "config_path = \"cfg/default.cfg\"\n",
    "config = ConfigLoader(config_path)\n",
    "\n",
    "print(\"\\033[34m-- Loaded Configuration --\\033[0m\")\n",
    "for attr, value in vars(config).items():\n",
    "    print(f\"{attr}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a0119e5b-a49d-4344-bdc7-f33cdb88000f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m-- Initialising training configuration\u001b[0m\n",
      "\u001b[34m-- Using device: cuda\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(\"\\033[34m-- Initialising training configuration\\033[0m\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "set_seed(config.seed)\n",
    "print(f\"\\033[34m-- Using device: {device}\\033[0m\")\n",
    "\n",
    "batch_size = config.batch_size\n",
    "train_pct = config.train_pct\n",
    "valid_pct = config.valid_pct\n",
    "n_classes = config.n_classes\n",
    "n_epochs = config.n_epochs\n",
    "\n",
    "input_dir = config.input_dir\n",
    "target_dir = config.target_dir\n",
    "output_dir = config.output_dir\n",
    "model_name = config.model_name\n",
    "model_save_dir = os.path.join(output_dir, \"saved_models\")\n",
    "os.makedirs(model_save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3418bf59-f9a0-41d7-8a00-fedd75d88c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m-- Loading data\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting Validation Set:  83%|████████▎ | 7921/9505 [02:38<00:30, 52.13it/s]"
     ]
    }
   ],
   "source": [
    "print(\"\\033[34m-- Loading data\\033[0m\")\n",
    "data_loader = ImageDataLoader(\n",
    "    input_dir=input_dir,\n",
    "    target_dir=target_dir,\n",
    "    batch_size=batch_size,\n",
    "    train_pct=train_pct,\n",
    "    valid_pct=valid_pct,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f043f6-f18b-441e-8d0c-c004dd9cb030",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[34m-- Loading or calculating class weights\\033[0m\")\n",
    "class_weights = load_class_weights()  \n",
    "if class_weights is None:  \n",
    "    train_stats = data_loader.count_classes(n_classes)\n",
    "    class_weights = get_class_weights(train_stats)\n",
    "    #save_class_weights(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f633e1-dd3c-4ab6-b89e-5b2e5a4e6c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[34m-- Initialising model, loss function, and optimiser\\033[0m\")\n",
    "\n",
    "importlib.reload(lib.loss)\n",
    "from lib.loss import FocalLoss\n",
    "\n",
    "print(class_weights)\n",
    "model, loss_fn, optim = create_model(n_classes, class_weights, device)\n",
    "model = model.to(device)\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, mode='min', factor=0.1, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d968f811",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "output_path = os.path.join(output_dir, f\"{model_name}_metrics.root\")\n",
    "with uproot.recreate(output_path) as output:\n",
    "    print(\"\\033[34m-- Starting training loop\\033[0m\")\n",
    "\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        epoch_train_loss = []\n",
    "        epoch_valid_loss = []\n",
    "        \n",
    "        for i, batch in enumerate(tqdm(data_loader.train_dl, desc=f\"Training Epoch {epoch+1}\")):\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            y = y.to(torch.long)\n",
    "            optim.zero_grad()\n",
    "\n",
    "            pred = model(x)\n",
    "            loss = loss_fn(pred, y)\n",
    "                    \n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            pred_labels = torch.argmax(pred, dim=1).cpu().numpy()\n",
    "            true_labels = y.cpu().numpy()\n",
    "            # Flatten the arrays\n",
    "            true_labels_flat = true_labels.reshape(-1)  # Flatten to 1D\n",
    "            pred_labels_flat = pred_labels.reshape(-1)  # Flatten to 1D\n",
    "            \n",
    "            # Ensure integer type\n",
    "            true_labels_flat = true_labels_flat.astype(int)\n",
    "            pred_labels_flat = pred_labels_flat.astype(int)\n",
    "\n",
    "\n",
    "            batch_accuracy = accuracy_score(true_labels_flat, pred_labels_flat)\n",
    "            batch_precision = precision_score(true_labels_flat, pred_labels_flat, average=\"weighted\", zero_division=0)\n",
    "            batch_recall = recall_score(true_labels_flat, pred_labels_flat, average=\"weighted\", zero_division=0)\n",
    "            batch_f1 = f1_score(true_labels_flat, pred_labels_flat, average=\"weighted\", zero_division=0)\n",
    "            \n",
    "            print(f\"Accuracy: {batch_accuracy:.4f}, Precision: {batch_precision:.4f}, Recall: {batch_recall:.4f}, F1-score: {batch_f1:.4f}\")\n",
    "\n",
    "            minority_class = 2\n",
    "            precision = precision_score(true_labels_flat, pred_labels_flat, labels=[minority_class], average=None, zero_division=0)[0]\n",
    "            recall = recall_score(true_labels_flat, pred_labels_flat, labels=[minority_class], average=None, zero_division=0)[0]\n",
    "\n",
    "            print(f\"precision {precision} and recall {recall}\")\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "            tqdm.write(f\"\\033[34m--- Epoch {epoch+1}, Batch {i+1} - Training Loss: {loss.item():.4f}, \"\n",
    "                       f\"Accuracy: {batch_accuracy:.4f}, Precision: {batch_precision:.4f}, \"\n",
    "                       f\"Recall: {batch_recall:.4f}, F1-score: {batch_f1:.4f}\\033[0m\")\n",
    "\n",
    "            for cls in range(n_classes):\n",
    "                cls_mask = (true_labels_flat == cls)\n",
    "                cls_accuracy = accuracy_score(true_labels_flat[cls_mask], pred_labels_flat[cls_mask])\n",
    "                print(f\"Class {cls} Accuracy: {cls_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                valid_iterator = iter(data_loader.valid_signature_dl)\n",
    "                try:\n",
    "                    random_val_batch = next(valid_iterator) \n",
    "                except StopIteration:\n",
    "                    valid_iterator = iter(data_loader.valid_signature_dl)\n",
    "                    random_val_batch = next(valid_iterator)\n",
    "\n",
    "                val_x, val_y = random_val_batch\n",
    "                val_x, val_y = val_x.to(device), val_y.to(device)\n",
    "\n",
    "                val_pred = model(val_x)\n",
    "                val_loss = loss_fn(val_pred, val_y)\n",
    "\n",
    "                valid_loss.append(val_loss.item())\n",
    "                tqdm.write(f\"\\033[34m--- Epoch {epoch+1}, Batch {i+1} - Validation Loss (Random Batch): {val_loss.item()}\\033[0m\")\n",
    "        \n",
    "            model.train()\n",
    "\n",
    "        model_save_path = os.path.join(model_save_dir, f\"{model_name}_epoch_{epoch+1}.pt\")\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"\\033[32m-- Model saved at {model_save_path}\\033[0m\")\n",
    "\n",
    "    output[\"training\"] = {\n",
    "        \"train_loss\": train_losses,\n",
    "        \"valid_loss\": valid_losses,\n",
    "    }\n",
    "\n",
    "    print(\"\\033[32m-- Training complete\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5943ad27-38d5-4156-a89f-dbfb0b0614c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9eedba-33c8-49e4-a473-a2d4df17ad3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb36b46-5972-4d43-a985-d0aa6db95965",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
